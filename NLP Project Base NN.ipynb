{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee8c541-1456-442f-a0de-e999b34a0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy --upgrade\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#!pip install \"gensim\"\n",
    "from gensim.models import Word2Vec   # Word2Vec model\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e893f275-a28a-433c-9bcf-723724eeeea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_wv = KeyedVectors.load('/home/x-mbemus/Desktop/train-text-50.vecs').wv   # Load in word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824f2177-25a6-4efd-9a97-f4c1c0083463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pickle-mixin\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3e93e0-9cc9-4c06-8d4a-8f8894ee267d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>genres</th>\n",
       "      <th>summary</th>\n",
       "      <th>link</th>\n",
       "      <th>is_novel</th>\n",
       "      <th>sum_len</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>6921</td>\n",
       "      <td>Carmilla</td>\n",
       "      <td>Sheridan Le Fanu</td>\n",
       "      <td>1872</td>\n",
       "      <td>fiction</td>\n",
       "      <td>The story is presented by Le Fanu as part of ...</td>\n",
       "      <td>https://www.gutenberg.org/files/10007/10007-0.txt</td>\n",
       "      <td>False</td>\n",
       "      <td>934</td>\n",
       "      <td>10007_10007-0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>7923</td>\n",
       "      <td>Dracula</td>\n",
       "      <td>Bram Stoker</td>\n",
       "      <td>1897</td>\n",
       "      <td>horror</td>\n",
       "      <td>The novel is told in epistolary format, as a ...</td>\n",
       "      <td>https://www.gutenberg.org/files/345/345-0.txt</td>\n",
       "      <td>True</td>\n",
       "      <td>986</td>\n",
       "      <td>345_345-0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>7923</td>\n",
       "      <td>Dracula</td>\n",
       "      <td>Bram Stoker</td>\n",
       "      <td>1897</td>\n",
       "      <td>horror</td>\n",
       "      <td>The novel is told in epistolary format, as a ...</td>\n",
       "      <td>https://www.gutenberg.org/files/45839/45839-0.txt</td>\n",
       "      <td>True</td>\n",
       "      <td>986</td>\n",
       "      <td>45839_45839-0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>8237</td>\n",
       "      <td>Don Quixote</td>\n",
       "      <td>Miguel de Cervantes</td>\n",
       "      <td>1605</td>\n",
       "      <td>novel</td>\n",
       "      <td>The First Sally Alonso Quijano, the protagoni...</td>\n",
       "      <td>https://www.gutenberg.org/files/996/996-0.txt</td>\n",
       "      <td>True</td>\n",
       "      <td>1592</td>\n",
       "      <td>996_996-0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>13535</td>\n",
       "      <td>Heart of Darkness</td>\n",
       "      <td>Joseph Conrad</td>\n",
       "      <td>1899</td>\n",
       "      <td>novel</td>\n",
       "      <td>'Heart of Darkness' opens in first person nar...</td>\n",
       "      <td>https://www.gutenberg.org/files/219/219-0.txt</td>\n",
       "      <td>True</td>\n",
       "      <td>3127</td>\n",
       "      <td>219_219-0.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id              title               author pub_date  \\\n",
       "0           2   6921           Carmilla     Sheridan Le Fanu     1872   \n",
       "1           3   7923            Dracula          Bram Stoker     1897   \n",
       "2           4   7923            Dracula          Bram Stoker     1897   \n",
       "3           5   8237        Don Quixote  Miguel de Cervantes     1605   \n",
       "4           9  13535  Heart of Darkness        Joseph Conrad     1899   \n",
       "\n",
       "    genres                                            summary  \\\n",
       "0  fiction   The story is presented by Le Fanu as part of ...   \n",
       "1   horror   The novel is told in epistolary format, as a ...   \n",
       "2   horror   The novel is told in epistolary format, as a ...   \n",
       "3    novel   The First Sally Alonso Quijano, the protagoni...   \n",
       "4    novel   'Heart of Darkness' opens in first person nar...   \n",
       "\n",
       "                                                link  is_novel  sum_len  \\\n",
       "0  https://www.gutenberg.org/files/10007/10007-0.txt     False      934   \n",
       "1      https://www.gutenberg.org/files/345/345-0.txt      True      986   \n",
       "2  https://www.gutenberg.org/files/45839/45839-0.txt      True      986   \n",
       "3      https://www.gutenberg.org/files/996/996-0.txt      True     1592   \n",
       "4      https://www.gutenberg.org/files/219/219-0.txt      True     3127   \n",
       "\n",
       "           file_name  \n",
       "0  10007_10007-0.txt  \n",
       "1      345_345-0.txt  \n",
       "2  45839_45839-0.txt  \n",
       "3      996_996-0.txt  \n",
       "4      219_219-0.txt  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/x-mbemus/Desktop/text_train_data.csv\")   # Import training data.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c1724b6-4d9d-4d69-831c-fc68b92879f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign classifier.\n",
    "base_nn = MLPClassifier(random_state=42, hidden_layer_sizes=(100, 100, 100), max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2653d8d-6f5e-4ad1-9809-56357588f6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.38713482,  0.35845155, -0.4892324 ,  0.10323673,  0.21779941,\n",
       "        0.18955876,  0.6996074 ,  0.22049618, -0.26312035,  0.28026226,\n",
       "        0.41193014, -0.18090388, -0.24702743,  0.483984  ,  0.01322602,\n",
       "        0.23626558,  0.3272087 , -0.00264295, -0.24421367, -0.5000111 ,\n",
       "        0.2437251 ,  0.67580444, -0.16511494,  0.21222222,  0.33947915,\n",
       "        0.04529685, -0.01860966,  0.05741313, -0.47610345,  0.07334362,\n",
       "        0.04463233, -0.32907084, -0.11431769, -0.44045985, -0.22120006,\n",
       "        0.1330758 ,  0.21142244,  0.14917983,  0.34179702,  0.4040718 ,\n",
       "        0.17557351, -0.05840934, -0.0798966 ,  0.17631772,  0.3998845 ,\n",
       "       -0.08932945,  0.17433514,  0.02371125, -0.05882114,  0.4896915 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_wv['Dracula']   # Check vectors loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d53067-8ba6-4693-babe-d90ed285f2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000,\n",
       "              random_state=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single observation to test on.\n",
    "f = open(\"/home/x-mbemus/Desktop/text_files/\" + df.at[0, \"file_name\"])\n",
    "text = ' '.join(f.readlines())   # Read in file.\n",
    "# Preprocess as before.\n",
    "text = re.sub(r\"\\\\n|\\\\t\", \" \", text)\n",
    "text = re.sub(r\"([^\\w\\s'])\", r\" \\1 \", text)\n",
    "text = re.sub(r\"\\s+\", \" \", text)\n",
    "words = text.split(\" \")   # Split tokens by space.\n",
    "\n",
    "text_vec = np.array([0] * 50)   # Create empty array.\n",
    "ttl_wrd = 0   # Count words.\n",
    "\n",
    "for j in words:   # For each word.\n",
    "    try:\n",
    "        text_vec = text_vec + tr_wv[j]   # Vectorize word and add to empty array.\n",
    "        ttl_wrd += 1   # Increment word count.\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "x_0 = text_vec/ttl_wrd   # Create mean vector.\n",
    "x_0 = x_0.tolist()   # Easier to use as list.\n",
    "\n",
    "text = df.at[0, \"summary\"]   # Read in summary.\n",
    "# Preprocess summary.\n",
    "text = re.sub(r\"\\\\n|\\\\t\", \" \", text)\n",
    "text = re.sub(r\"\\\\'\", \" \", text)\n",
    "text = re.sub(r\"([^\\w\\s'])\", r\" \\1 \", text)\n",
    "text = re.sub(r\"\\s+\", \" \", text)\n",
    "words = text.split(\" \")   # Split tokens by space.\n",
    "# Create first observation input.\n",
    "inp = [0] * 150   # Empty tri-gram.\n",
    "inp = x_0 + inp + [words[0]]   # Mean vector, tri-gram, target output.\n",
    "full_inp = pd.DataFrame(np.array(inp).reshape(1, len(inp)))   # Convert to data frame.\n",
    "\n",
    "for j in range(1, len(words)):   # For each word in summary.\n",
    "    try:\n",
    "        inp = tr_wv[j-1].tolist()   # Try to convert last word to vector.\n",
    "    except:\n",
    "        inp = [0] * 50   # If not in vectors, use empty vector instead.\n",
    "        \n",
    "    # Mean vector, previous word, two words before that, target word. \n",
    "    inp = x_0 + inp + list(full_inp.iloc[j-1, 50:150]) + [words[j]]  \n",
    "    inp = pd.DataFrame(np.array(inp).reshape(1, len(inp)))   # Convert observation to DF.\n",
    "    full_inp = pd.concat([full_inp, inp]).reset_index(drop=True)   # Add to df.\n",
    "\n",
    "full_inp.replace([\"\", np.nan], 0)   # Replace all empty values with NA's.\n",
    "\n",
    "# Data-type dictionary.\n",
    "d_dict = {}\n",
    "for j in range(0, 200):   # First 200 should be floats.\n",
    "    d_dict[j] = float\n",
    "d_dict[200] = str   # Last should be string.\n",
    "\n",
    "full_inp = full_inp.astype(d_dict)   # Convert full df to dictionary.\n",
    "\n",
    "base_nn.fit(full_inp.iloc[:, 0:199], full_inp.loc[:, 200])   # Train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ffef8ca-a190-4fdf-8d14-965ef065aae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.1474203   1.5263748  -1.7495593   3.5340645   1.0957179   2.441623\n",
      "  2.2997258  -3.2771723  -0.03800014 -2.2002282   2.6911736   4.5104384\n",
      " -2.5005765   3.8492079   1.3173671   1.2771536   1.0527943   1.4081445\n",
      " -1.292123    1.3697222   1.0598265   0.6523853  -0.10165195  3.3674347\n",
      " -3.9011934  -2.0526927   1.0369684  -2.7872827  -1.4726132   4.078271\n",
      "  2.3434234  -0.31009197  3.7382123  -6.4433727   2.3364267  -1.0388134\n",
      "  0.00880193  2.809391   -0.07413822  2.4750824   1.2271886   1.594003\n",
      " -0.7169712  -1.7197332   4.200405   -1.7136725   3.305261   -2.1560016\n",
      " -1.0708046   0.83876747]\n"
     ]
    }
   ],
   "source": [
    "print(tr_wv[\"\"])   # Test embedding for empty.\n",
    "#words[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce518ab2-cfe3-4daa-beab-cb7ace16eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.index:   # Read in texts to data frame.\n",
    "    try:\n",
    "        # Open text file.\n",
    "        f = open(\"/home/x-mbemus/Desktop/text_files/\" + df.at[i, \"file_name\"], \"r\", errors=\"ignore\")\n",
    "        text = ' '.join(f.readlines())   # Read in text.\n",
    "        # Preprocess as always.\n",
    "        text = re.sub(r\"\\\\n|\\\\t\", \" \", text)\n",
    "        text = re.sub(r\"([^\\w\\s'])\", r\" \\1 \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        \n",
    "        # Add to data frame.\n",
    "        df.at[i, \"full_text\"] = text\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90e5d127-0598-46a5-be06-e76f195af10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666\n",
      "553\n"
     ]
    }
   ],
   "source": [
    "# Check effect of dropping, then drop.\n",
    "print(len(df.index))\n",
    "print(len(df.loc[:]['full_text'].dropna().index))\n",
    "\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27bd9c7c-9607-4206-9971-e7a3a72ea458",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.index:   # Preprocess summaries.\n",
    "    text = df.at[i, \"summary\"]   # Read in summaries.\n",
    "    # Preprocess as before.\n",
    "    text = re.sub(r\"\\\\n|\\\\t\", \" \", text)\n",
    "    text = re.sub(r\"\\\\'\", \" \", text)\n",
    "    text = re.sub(r\"([^\\w\\s'])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    df.at[i, \"summary\"] = text   # Add summary to df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eaef3fe-3c18-4492-a66a-81e90c08f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)   # Reset index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5010cb68-5f70-4687-94e0-e3b904b515c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_embedding(start, stop):   # Function to create embeddings given index.\n",
    "    \n",
    "    global df   # Import df.\n",
    "    \n",
    "    # First observation.\n",
    "    words = df.at[start,\"full_text\"].split(\" \")   # Collect tokens of text.\n",
    "\n",
    "    text_vec = np.array([0] * 50)   # Empty array.\n",
    "    ttl_wrd = 0    # Empty count.\n",
    "\n",
    "    for j in words:\n",
    "        try:\n",
    "            text_vec = text_vec + tr_wv[j]   # Vectorize and add.\n",
    "            ttl_wrd += 1   # Increment count.\n",
    "        except:    # Ignore if can't vectorize.\n",
    "            pass\n",
    "\n",
    "    x_0 = text_vec/ttl_wrd   # Compute mean.\n",
    "    x_0 = x_0.tolist()    # Convert to list.\n",
    "\n",
    "    words = df.at[start, \"summary\"].split(\" \")    # Import summary.\n",
    "    inp = [0] * 150   # Empty trigram.\n",
    "    inp = x_0 + inp + [words[0]]   # mean vector, trigram, output word.\n",
    "    full_inp = pd.DataFrame(np.array(inp).reshape(1, len(inp)))   # Create data frame of inputs.\n",
    "\n",
    "    for j in range(1, len(words)):   # For each other word.\n",
    "        try:\n",
    "            inp = tr_wv[j-1].tolist()   # Convert last word to vector.\n",
    "        except:\n",
    "            inp = [0] * 50   # If can't vectorize, impute empty.\n",
    "        # Mean vector, last word, two words prior to last, output word.\n",
    "        inp = x_0 + inp + list(full_inp.iloc[j-1, 50:150]) + [words[j]]\n",
    "        inp = pd.DataFrame(np.array(inp).reshape(1, len(inp)))   # Convert to df.\n",
    "        full_inp = pd.concat([full_inp, inp]).reset_index(drop=True)   # Concatenate all.\n",
    "    \n",
    "    for i in range(start+1, stop):     # For every other file in range.\n",
    "        # Repeat process.\n",
    "        # Open tokens of text.\n",
    "        words = df.at[i,\"full_text\"].split(\" \")\n",
    "        \n",
    "        # Conmpute mean vector as before.\n",
    "        text_vec = np.array([0] * 50)\n",
    "        ttl_wrd = 0\n",
    "\n",
    "        for j in words:\n",
    "            try:\n",
    "                text_vec = text_vec + tr_wv[j]\n",
    "                ttl_wrd += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        x_0 = text_vec/ttl_wrd\n",
    "        x_0 = x_0.tolist()\n",
    "\n",
    "        # Read in summary tokens..\n",
    "        words = df.at[i, \"summary\"].split(\" \")\n",
    "        # Create first input vector as before.\n",
    "        inp = [0] * 150\n",
    "        inp = x_0 + inp + [words[0]]\n",
    "        # Sub-DF to hold sentences of this text.\n",
    "        sub_inp = pd.DataFrame(np.array(inp).reshape(1, len(inp)))\n",
    "        \n",
    "        # Process every word after first as before.\n",
    "        for j in range(1, len(words)):\n",
    "            try:\n",
    "                inp = tr_wv[words[j-1]].tolist()\n",
    "            except:\n",
    "                inp = [0] * 50\n",
    "            inp = x_0 + inp + list(sub_inp.iloc[j-1, 50:150]) + [words[j]]\n",
    "            inp = pd.DataFrame(np.array(inp).reshape(1, len(inp)))\n",
    "            sub_inp = pd.concat([sub_inp, inp]).reset_index(drop=True)\n",
    "            \n",
    "        # Combine sub-DF to full DF.\n",
    "        full_inp = pd.concat([full_inp, sub_inp]).reset_index(drop=True)\n",
    "        \n",
    "        # Print current process.\n",
    "        clear_output(wait=True)\n",
    "        print(f\"{i+1}/{len(df.index)} Complete\")\n",
    "    \n",
    "    # Save train df as pkl object.\n",
    "    with open(f'/home/x-mbemus/Desktop/Checkpoints/NN_Base_train{stop}.pkl','wb') as f:\n",
    "        pkl.dump(full_inp,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d713b9-d879-40bf-9d44-172ad39a60f1",
   "metadata": {},
   "source": [
    "## Running Model!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "381af2d2-216d-4c71-87b5-9a3de5f1d95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553/553 Complete\n"
     ]
    }
   ],
   "source": [
    "# Embedd within each range.\n",
    "ends = [[0, 50], [50, 100], [100, 150], [150, 200], [200, 250], [250, 300], \n",
    "        [300, 350], [350, 400], [400, 450], [450, 500], [500, 553]]\n",
    "\n",
    "for end in ends:\n",
    "    batch_embedding(end[0], end[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c00f2d2-f380-4d40-a49a-8311d55c2a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/553 Complete\n"
     ]
    }
   ],
   "source": [
    "batch_embedding(0, 50)   # Testing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1634660e-08c0-45a5-a12d-603f83ef900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/home/x-mbemus/Desktop/Checkpoints/NN_Base_train50.pkl','rb') as f:\n",
    "    tr_data = pkl.load(f)   # Load first 50 input sets.\n",
    "\n",
    "for i in [100, 150, 200, 250, 300, 350, 400, 450, 500, 553]:\n",
    "    with open(f'/home/x-mbemus/Desktop/Checkpoints/NN_Base_train{i}.pkl','rb') as f:\n",
    "        sub_data = pkl.load(f)   # Load each other input set.\n",
    "        tr_data = pd.concat([tr_data, sub_data]).reset_index(drop=True)   # Concatenate consecutively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f82e3d34-3580-4585-accd-9d0d20d0aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign d-type as before. \n",
    "d_dict = {}\n",
    "for j in range(0, 200): \n",
    "    d_dict[j] = float \n",
    "    d_dict[200] = str\n",
    "\n",
    "tr_data = tr_data.astype(d_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9877d40-b167-470e-b979-caeac8917f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Classifier.\n",
    "base_nn = MLPClassifier(random_state=42, hidden_layer_sizes=(100, 100, 100), max_iter=1000)\n",
    "\n",
    "# Train classifier.\n",
    "base_nn.fit(tr_data.iloc[:, 0:200], tr_data.iloc[:, 200])\n",
    "\n",
    "# Save classifier.\n",
    "with open(f'/home/x-mbemus/Desktop/Checkpoints/NN_Base_real.pkl','wb') as f: \n",
    "    pkl.dump(base_nn,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e411bf-5d1d-44c8-bf7a-963b450c3081",
   "metadata": {},
   "source": [
    "base_nn = MLPClassifier(random_state=42, hidden_layer_sizes=(100, 100, 100), max_iter=1000)\n",
    "\n",
    "d_dict = {}\n",
    "for j in range(0, 200):\n",
    "    d_dict[j] = float\n",
    "    d_dict[200] = str\n",
    "\n",
    "\n",
    "words = df.at[0,\"full_text\"].split(\" \")\n",
    "\n",
    "text_vec = np.array([0] * 50)\n",
    "ttl_wrd = 0\n",
    "\n",
    "for j in words:\n",
    "    try:\n",
    "        text_vec = text_vec + tr_wv[j]\n",
    "        ttl_wrd += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "x_0 = text_vec/ttl_wrd\n",
    "x_0 = x_0.tolist()\n",
    "\n",
    "words = df.at[0, \"summary\"].split(\" \")\n",
    "inp = [0] * 150\n",
    "inp = x_0 + inp + [words[0]]\n",
    "full_inp = pd.DataFrame(np.array(inp).reshape(1, len(inp)))\n",
    "\n",
    "for j in range(1, len(words)):\n",
    "    try:\n",
    "        inp = tr_wv[j-1].tolist()\n",
    "    except:\n",
    "        inp = [0] * 50\n",
    "    inp = x_0 + inp + list(full_inp.iloc[j-1, 50:150]) + [words[j]]\n",
    "    inp = pd.DataFrame(np.array(inp).reshape(1, len(inp)))\n",
    "    full_inp = pd.concat([full_inp, inp]).reset_index(drop=True)\n",
    "    \n",
    "for i in range(1, len(df.index)):  \n",
    "    #try:\n",
    "        words = df.at[i,\"full_text\"].split(\" \")\n",
    "\n",
    "        text_vec = np.array([0] * 50)\n",
    "        ttl_wrd = 0\n",
    "\n",
    "        for j in words:\n",
    "            try:\n",
    "                text_vec = text_vec + tr_wv[j]\n",
    "                ttl_wrd += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        x_0 = text_vec/ttl_wrd\n",
    "        x_0 = x_0.tolist()\n",
    "\n",
    "        words = df.at[i, \"summary\"].split(\" \")\n",
    "        inp = [0] * 150\n",
    "        inp = x_0 + inp + [words[0]]\n",
    "        sub_inp = pd.DataFrame(np.array(inp).reshape(1, len(inp)))\n",
    "\n",
    "        for j in range(1, len(words)):\n",
    "            try:\n",
    "                inp = tr_wv[words[j-1]].tolist()\n",
    "            except:\n",
    "                inp = [0] * 50\n",
    "            inp = x_0 + inp + list(sub_inp.iloc[j-1, 50:150]) + [words[j]]\n",
    "            inp = pd.DataFrame(np.array(inp).reshape(1, len(inp)))\n",
    "            sub_inp = pd.concat([sub_inp, inp]).reset_index(drop=True)\n",
    "            \n",
    "        full_inp = pd.concat([full_inp, sub_inp]).reset_index(drop=True)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print(f\"{i+1}/{len(df.index)} Complete\")\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            with open(f'/home/x-mbemus/Desktop/Checkpoints/NN_Base_train{i}.pkl','wb') as f:\n",
    "                pkl.dump(full_inp,f)\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "with open(f'/home/x-mbemus/Desktop/Checkpoints/NN_Base_train_FULL.pkl','wb') as f:\n",
    "    pkl.dump(full_inp,f)\n",
    "    \n",
    "full_inp.replace([\"\", np.nan], 0)\n",
    "\n",
    "full_inp = full_inp.astype(d_dict) \n",
    "\n",
    "print(\"Training Model...\")\n",
    "\n",
    "base_nn.fit(full_inp.iloc[:, 0:200], full_inp.loc[:, 200])\n",
    "\n",
    "with open(f'/home/x-mbemus/Desktop/Checkpoints/NN_Base_full.pkl','wb') as f:\n",
    "            pkl.dump(base_nn,f)\n",
    "        \n",
    "print(\"Training Complete!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e02581f-b844-4ac4-89b6-d8d802407c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346664f-98ef-45d5-94f0-4e1c26ca5f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0619d919-5adb-497c-bb2f-674e52bb2b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open classifier.\n",
    "with open('/home/x-mbemus/Desktop/Checkpoints/NN_Base_real.pkl', 'rb') as f:\n",
    "    base_nn = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f5df453-ee06-49f5-a20a-178dc2839551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics.\n",
    "from sacrebleu.metrics import BLEU\n",
    "bleu_scorer = BLEU(effective_order=True)\n",
    "\n",
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cdb891a-98fb-4bc3-913d-b4c100bb389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/home/x-mbemus/Desktop/text_test_data.csv\")   # Open test df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255a6440-416d-4ca4-8608-40c59789064d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1034 Words Generated...\n",
      "The novel is a direct theme of the elves, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the emaciated, and the crew are greeted by the\n",
      "BLEU = 0.16 1.8/0.3/0.0/0.0 (BP = 1.000 ratio = 8.914 hyp_len = 1034 ref_len = 116)\n",
      "[{'rouge-1': {'r': 0.06172839506172839, 'p': 0.3333333333333333, 'f': 0.10416666402994797}, 'rouge-2': {'r': 0.020202020202020204, 'p': 0.11764705882352941, 'f': 0.03448275611920352}, 'rouge-l': {'r': 0.04938271604938271, 'p': 0.26666666666666666, 'f': 0.08333333069661467}}]\n"
     ]
    }
   ],
   "source": [
    "# Single test observations.\n",
    "h = 1   # Observation 1.\n",
    "# Read in file.\n",
    "f = open(\"/home/x-mbemus/Desktop/text_files/\" + test_df.at[h, \"file_name\"], \"r\", errors=\"ignore\")\n",
    "\n",
    "# Preprocess text.\n",
    "text = ' '.join(f.readlines())\n",
    "text = re.sub(r\"\\\\n|\\\\t\", \" \", text)\n",
    "text = re.sub(r\"([^\\w\\s'])\", r\" \\1 \", text)\n",
    "text = re.sub(r\"\\s+\", \" \", text)\n",
    "words = text.split(\" \")\n",
    "\n",
    "# Create mean vector.\n",
    "text_vec = np.array([0] * 50)\n",
    "ttl_wrd = 0\n",
    "\n",
    "for j in words:\n",
    "    try:\n",
    "        text_vec = text_vec + tr_wv[j]\n",
    "        ttl_wrd += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "x_0 = text_vec/ttl_wrd\n",
    "x_0 = x_0.tolist()\n",
    "\n",
    "# Read in target summary.\n",
    "target = test_df.at[h, \"summary\"]\n",
    "# Empty vector.\n",
    "inp = [0] * 100\n",
    "inp = x_0 + tr_wv[''].tolist() + inp   # Been using \"\" as start and end token.\n",
    "inp = pd.DataFrame(np.array(inp).reshape(1, len(inp)))   # Create df from first observation.\n",
    "        \n",
    "y_i = base_nn.predict(inp.iloc[:, 0:200])[0]   # Do first predition.\n",
    "full_output = y_i   # String for full output of test..\n",
    "\n",
    "itera = 1   # Iterable.\n",
    "while y_i != '' and itera < 1034:   # If not stop token and less than max summary length.\n",
    "    try:   # Convert last word to vector. \n",
    "        x_1 = tr_wv[y_i]\n",
    "        x_1 = x_1.tolist()\n",
    "        if len(x_1) == 1:   # Sometimes would create list with 1 observation.\n",
    "            x_1 = x_1[0]   # Extract vector from outer list. \n",
    "    except:\n",
    "        x_1 = [0] * 50   # Impute empty if conversion impossible.\n",
    "    inp = x_0 + x_1 + list(inp.iloc[0, 50:150])   # Mean vec, last vec, two vecs prior.\n",
    "    inp = pd.DataFrame(np.array(inp).reshape(1, len(inp)))   # Convert to df.\n",
    "            \n",
    "    y_i = base_nn.predict(inp.iloc[:, 0:200])[0]   # Predict with model.\n",
    "    if y_i in [\".\", \",\", \"!\", \"?\"]:   # If punctuation, just add.\n",
    "        full_output = full_output + y_i\n",
    "    else:\n",
    "        full_output = full_output + \" \" + y_i   # Otherwise, add with space.\n",
    "        \n",
    "    itera += 1   # Increment iterable.\n",
    "    # Check progress.\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{itera} Words Generated...\")\n",
    "        \n",
    "# Remove extra spacing on ends.\n",
    "full_output = full_output.strip()\n",
    "\n",
    "# Compute scores.\n",
    "blue = bleu_scorer.sentence_score(hypothesis=full_output, references=[target])\n",
    "red = rouge_scorer.get_scores(hyps=full_output, refs=target)\n",
    "        \n",
    "# Print sample output.\n",
    "print(full_output)\n",
    "print(blue)\n",
    "print(red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe19a6c1-ec04-4732-bdea-e72fe894d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of results.\n",
    "results = {\"title\":[], \"target\":[], \"pred\":[], \"BLEU\":[], \"ROUGE\":[]}\n",
    "\n",
    "exceptions = []   # List of exceptions.\n",
    "\n",
    "# Repeat process in cell above for each test observation.\n",
    "for h in test_df.index:\n",
    "    try:\n",
    "        f = open(\"/home/x-mbemus/Desktop/text_files/\" + test_df.at[h, \"file_name\"], \"r\", errors=\"ignore\")\n",
    "    \n",
    "        text = ' '.join(f.readlines())\n",
    "        text = re.sub(r\"\\\\n|\\\\t\", \" \", text)\n",
    "        text = re.sub(r\"([^\\w\\s'])\", r\" \\1 \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        words = text.split(\" \")\n",
    "\n",
    "        text_vec = np.array([0] * 50)\n",
    "        ttl_wrd = 0\n",
    "\n",
    "        for j in words:\n",
    "            try:\n",
    "                text_vec = text_vec + tr_wv[j]\n",
    "                ttl_wrd += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        x_0 = text_vec/ttl_wrd\n",
    "        x_0 = x_0.tolist()\n",
    "        \n",
    "        target = test_df.at[h, \"summary\"]\n",
    "        \n",
    "        inp = [0] * 100\n",
    "        inp = x_0 + tr_wv[''].tolist() + inp\n",
    "        inp = pd.DataFrame(np.array(inp).reshape(1, len(inp)))\n",
    "        \n",
    "        y_i = base_nn.predict(inp.iloc[:, 0:200])[0]\n",
    "        full_output = y_i\n",
    "        \n",
    "        out_len = 1\n",
    "        while y_i != '' and out_len < 1034:\n",
    "            try:\n",
    "                x_1 = tr_wv[y_i]\n",
    "                x_1 = x_1.tolist()\n",
    "                if len(x_1) == 1:\n",
    "                    x_1 = x_1[0]\n",
    "            except:\n",
    "                x_1 = [0] * 50\n",
    "            inp = x_0 + x_1 + list(inp.iloc[0, 50:150])\n",
    "            inp = pd.DataFrame(np.array(inp).reshape(1, len(inp)))\n",
    "            \n",
    "            y_i = base_nn.predict(inp.iloc[:, 0:200])[0]\n",
    "            if y_i in [\".\", \",\", \"!\", \"?\"]:\n",
    "                full_output = full_output + y_i\n",
    "            else:\n",
    "                full_output = full_output + \" \" + y_i\n",
    "            \n",
    "            out_len += 1\n",
    "        \n",
    "        full_output = full_output.strip()\n",
    "        \n",
    "        blue = bleu_scorer.sentence_score(hypothesis=full_output, references=[target])\n",
    "        \n",
    "        red = rouge_scorer.get_scores(hyps=full_output, refs=target)\n",
    "        \n",
    "        # Save results to result dictionary.\n",
    "        results[\"title\"].append(test_df.at[h, \"title\"])\n",
    "        results[\"target\"].append(target)\n",
    "        results[\"pred\"].append(full_output)\n",
    "        results[\"BLEU\"].append(blue.score)\n",
    "        results[\"ROUGE\"].append(red[0][\"rouge-l\"][\"f\"])\n",
    "    \n",
    "        test_df.at[h, \"flagged\"] = 0   # Do not flag.\n",
    "        \n",
    "    except Exception as e:   # If issues arose, flag.\n",
    "        exceptions.append((h, e))\n",
    "        test_df.at[h, \"flagged\"] = 1\n",
    "    \n",
    "    # Check progress.\n",
    "    clear_output(wait=True)\n",
    "    print(f'{h+1}/{len(test_df.index)}')\n",
    "\n",
    "test_results = pd.DataFrame(results)   # Convert dict to df.\n",
    "# Save df.\n",
    "test_results.to_csv(\"/home/x-mbemus/Desktop/BASENN_test_result2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db0b8169-dc20-4b30-9b82-65c2fbbd22ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38599607270217895\n",
      "0.054923850539211366\n"
     ]
    }
   ],
   "source": [
    "# Check results.\n",
    "print(test_results['BLEU'].mean())\n",
    "print(test_results['ROUGE'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff212cdd-b78d-45fb-b305-eee79aaa6b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel is extremely unfavorable. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500. The story is set approximately 500\n"
     ]
    }
   ],
   "source": [
    "print(test_results.at[0, \"pred\"])    # Test observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ec65ee-5eda-4903-bba6-3a81423f3474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666/666 Summaries Checked\n",
      "Current Max: 1034\n"
     ]
    }
   ],
   "source": [
    "# Find max summary length.\n",
    "maximum = 0   # Min length.\n",
    "for i in df.index:   # Iterate through df.\n",
    "    text = df.at[0, \"summary\"]   # Extract summary.\n",
    "    # Preprocess as before.\n",
    "    text = re.sub(r\"\\\\n|\\\\t\", \" \", text)\n",
    "    text = re.sub(r\"\\\\'\", \" \", text)\n",
    "    text = re.sub(r\"([^\\w\\s'])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    words = text.split(\" \")   # Split by space.\n",
    "    \n",
    "    if len(words) > maximum:   # If longest summary so far.\n",
    "        maximum = len(words)   # Assign as new max.\n",
    "    \n",
    "    # Track progress.\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{i+1}/{len(df.index)} Summaries Checked\")\n",
    "    print(f\"Current Max: {maximum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53387b3f-c7fc-431e-bef8-da2d8c870d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (Anaconda 2021.05)",
   "language": "python",
   "name": "anaconda-2021.05-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
